This course will teach the "magic" of getting deep learning to work well. Rather than the deep learning process being a black box, will 
understand what drives performance, and be able to more systematically get good results.
Understand industry best-practices for building deep learning applications. Be able to effectively use the common neural network "tricks"
, including initialization, L2 and dropout regularization, Batch normalization, gradient checking. Be able to implement and apply 
a variety of optimization algorithms, such as mini-batch gradient descent, Momentum, RMSprop and Adam, and check for their convergence.
Understand new best-practices for the deep learning era of how to set up train/dev/test sets and analyze bias/variance.

Below are the overview of projects as part of this course. Coded in numpy and pandas to clearly understand what's happening in background.

* [Initialization](https://github.com/raviteja-ganta/Deep-Learning-Specialization-Coursera/blob/master/Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Practical%20aspects%20of%20Deep%20Learning/Initialization.ipynb) - In this notebook,  will see how different initializations for coefficients lead to different results.
* [Regularization](https://github.com/raviteja-ganta/Deep-Learning-Specialization-Coursera/blob/master/Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Practical%20aspects%20of%20Deep%20Learning/Regularization.ipynb) - Use regularization in your deep learning models.
* [Optimization Methods](https://github.com/raviteja-ganta/Deep-Learning-Specialization-Coursera/blob/master/Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Optimization%20algorithms/Optimization%2Bmethods.ipynb) - In this project, will learn more advanced optimization methods that can speed up learning and perhaps even get us to a better final value for the cost function.
